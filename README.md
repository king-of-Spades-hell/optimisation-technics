# Optimization Algorithms in Python

This project implements unconstrained optimization algorithms from scratch using Python. The goal is to provide an intuitive understanding of these algorithms through detailed implementation and explanations.

## ðŸš€ Features
- All algorithms implemented from scratch.
- Step-by-step explanation for each algorithm.
- Easy-to-read code with detailed comments.
- Suitable for beginners and advanced learners.

## ðŸ“š Included Algorithms

### Gradient-Based Optimization
#### Gradient Descent
Iteratively updates parameters in the direction of the negative gradient to minimize the objective function.

#### Momentum Gradient Descent
Accelerates convergence using momentum to escape local minima and optimize the search direction.

#### Nesterov Accelerated Gradient
Improves upon Momentum Gradient Descent by adjusting gradients based on a lookahead point.

### Second-Order Optimization
#### Newton's Method
Leverages second-order derivatives (Hessian) for faster convergence towards local minima.

#### Quasi-Newton Methods
Approximates the Hessian for optimization without computing it explicitly.


## ðŸ“Š Visualization Plots are used to visualize:

-Convergence over iterations.
-Contour plots for 2D functions.
-Learning rate effect on optimization performance.

---

Stay tuned for more algorithms and enhancements!
